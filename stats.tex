\documentclass[10pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{bm}
\graphicspath{{images/}}

\newcommand{\R}{\mathbb{R}}

\begin{document}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]


	\section{Basic definitions}

	\begin{defn}{\textbf{Support}}
		The set of values a random variable can take (set of all the numerical realizations of outcomes) e.g. for a binary random variable X, it's support is $R_X = \{ 0, 1 \}$
	\end{defn}


	\begin{defn}{\textbf{Sample Point}}
		In an experiment, a sample point is one of the possible outcomes of experiment denoted by $\omega$
	\end{defn}

	\begin{defn}{\textbf{Independent Events}}
	Two events A and B are independent if and only if \\
	\begin{align}
		P(A \cap B) = P(A) * P(B)
	\end{align}
	Knowing that B has occured, does not make us change our belief of the probabilities of A, and vice versa.
	\end{defn}

	\begin{defn}{\textbf{Jointly/Mutually Independent Events}}
	Let $E_1, ... , E_n$ be n events. \\
	$E_1, ... , E_n$ are jointly independent(or mutually independent)
	if and only if for any sub-collection of k events $(k \leq n) E_{i1}, ... , E_{ik}$:
	\begin{align}
	P(\bigcap \limits_{j=1}^kE_{ij}) = \prod_{j=1}^{k}P(E_{ij})
	\end{align}
	\end{defn}

	\begin{defn}{\textbf{Conditional Independence of Events}}
		Let there be three events: $A, B, C$,
		We say that $B$ and $C$ are independent, given the condition that event $A$ has occured.
		Before knowing that $A$ has occured, they might not have been independent.
	\end{defn}

	\begin{defn}{\textbf{Random Variable}}
		A random Variable $X$ is a function from sample space $\Omega$ to set of real numbers $\R$, i.e $ X: \Omega -> \R $.\\
		
		The real number $X(\omega)$ associated with sample point $\omega \in \Omega$ is called a realization of the random variable. The set of all possible realizations is called the support and denoted by $R_X$
	\end{defn}

	\begin{defn}{\textbf{Independence of Random Variables}}
		Two random Variables $X$ and $Y$ are independent of each other, if the joint PMF of X and Y,
		satisfies following condition:
		\begin{align}
			p_{X,Y} = p_X(x) \cdot p_Y(y) \quad is\ true\ for\  \forall x \in X, y \in Y
		\end{align}
		Understanding it in simple terms, for the 2-d table for PMF, any entry can be known by 
		multiplying corresponding values in marginal probabilities $p_X(x)$ and $p_Y(y)$, if they are independent, and vice versa.
	\end{defn}

	\begin{defn}{\textbf{Probability Mass Function}}
		The PMF of a discrete random variable $X$ is a function $p_X : \R -> [0,1]$ such that\\
		\begin{align}
			p_X = P(X = x)\quad \forall x \in \R
		\end{align}
		where $P(X = x)$ is the probability of realization of random variable X will be equal to x. Basically PMF is numerical realizations $->$ respective Probabilities.
	\end{defn}

	\begin{defn}{\textbf{Distribution Function/Cumulative Distribution Function}}
		If $X$ is a random variable, its distribution/cdf is a function $F_X : \R -> [0,1]$ such that \\
		\begin{align}
			F_X(x) = P(X \le x) \quad \forall x \in \R
		\end{align}
	\end{defn}

	\begin{defn}{\textbf{Expected Value of a Random Variable}}
	The expected value of random variable $X$ is the weighted average of the values that X can take on where each possible realization value is weighted by its respective probability i.e. \\
	\begin{align}
		\mathbf{E}[X] = \sum\limits_{x \in R_X}xp_X(x)
	\end{align}
	
	In other words, it is the weighted average of all possible numerical outcomes(in the support) widghted with respect to their respective probabilities
	\end{defn}

	\begin{defn}{\textbf{Expected value of function of Random Variable}}
		Expected value of function $g(Y)$ say
		$g(Y) = Y^2$:
		\begin{align}
			\mathbf{E}[g(Y)] = \sum_{y \in R_Y}g(y)p(y)
		\end{align}
	\end{defn}

	\begin{defn}{\textbf{Linearity of Expectation}}
		If $X$ is a random variable and $a \in R$ is a constant, then
		\begin{align}
			\mathbf{E}[aX] = a\mathbf{E}[X]
		\end{align}
		In a more general setting:\\
		If $Y$ is a random variable such that $Y = a + bX$, where 
		$X$ is a random variable, and $a$ and $b$ are constants,
		\begin{align}
			\begin{split}
				\mathbf{E}[Y] & = \mathbf{E}[a] + \mathbf{E}[bX] \\
				     & = a + b\mathbf{E}[X] \\
				\mathbf{E}[Y] & = a + b\mathbf{E}[X] \\
			\end{split}
		\end{align}
		
	\end{defn}

	\begin{defn}{\textbf{Conditional expectation of Random Variable}}
		Given random variables $X$ and $Y$, The conditional expectation of $X$ given $Y = y$ is the weighted average of values that X can take on, where each possible value is weighted by its respective conditional probability (conditinal on the information Y = y), denoted by $E[X|Y = y]$
		\begin{align}
			\mathbf{E}[X|Y = y] = \sum_{x \in R_x} xp_{X|Y = y}(x)
		\end{align}
		
		Another fact worth noting is that $\mathbf{E}[X|Y=y]$ when y is known is a number, but when y is unknown, $\mathbf{E}[X|Y]$ acts like a random variable that is a function of random variable Y.i.e. $\mathbf{E}[X|Y] = g(Y)$
	\end{defn}

	\begin{defn}{\textbf{Total Expectation/Iterated Expectation}}
		The law of total expectation/law of iterated expectation/tower rule/smoothing theorem/adam's law among other names states that if,
		$X$ is a random variable whose expected value $E[X]$ is defined, and $Y$ is any random variable on same probability space then
		\begin{align}
			\mathbf{E}[X] = \mathbf{E}[\mathbf{E}[X|Y]]
		\end{align}
		This comes from the fact that if there are $A_i$ is a finite countable partition of the sample space,
		then
		\begin{align}
			\mathbf{E}[X] = \sum_{i}\mathbf{E}[X|A_i]P[A_i]
		\end{align}
		In other words, the first equation is also summarized as, the expected value of a conditional expectation (an r.v.) is its unconditional expected value.
	\end{defn}

	\begin{defn}{\textbf{Geometric Random Variable}}
		Given that phenomena has a sequence of independent trials, there are two possible outcomes for each trial and the probaility of success p is same for every trial, \\
		The geometric random variable $X$ is the count of number of failures before the first success.
		i.e. \\ 
		$Pr(X = k)$ means the first success happened on the $k^{th}$ trial.
		The support $R_X={1,2,3..}$.  Don't confuse this with the outcomes of a single trial. We should really think that experiment is an aggregate process.\\
		The \textbf{pmf of X} is looks like exponentially decreasing probability that adds up in total to 1.
		\begin{align}
			P(X = k) = (1-p)^{k}p
		\end{align}
		
	\end{defn}

	\begin{defn}{\textbf{Quantile}}
	The $p-$Quantile of a Random Variable $X$ is given $p \in (0,1)$
	and $X$ having cumulative distribution function(cdf) to be $F_X(x)$, the $p-$Quantile is:
	\begin{align}
	Q_X(p) = min \{ x \in R : F_X(x) \ge p \}
	\end{align}
	This translates roughtly equivalently to the definition of percentiles. e.g 0.5-quantile is a median. 0.25-Quantiles are quartiles etc.
	\end{defn}

	\begin{defn}{\textbf{Deviation of a Random Variable}}
	The Deviation of a Random Variable is its difference from its mean value/Expected value. It is denoted by 
	\begin{align}
		\overline{X} = X - \mathbf{E}[X]
	\end{align}
	Deviation of Random Variable is also a random variable in it's
	own respect.
	\end{defn}

	\begin{defn}{\textbf{Variance}}
		It is a measure of dispersion of a random variable (expected squared deviation from expectation). Let $X$ be a random variable. The variance of X, denoted by $Var[X]$ is defined as follows:\\
		\begin{align}
			\mathbf{Var}[X] = \mathbf{E}[(X - \mathbf{E}[X])^2] = \mathbf{E}[\overline{X}^2]
			 = \mathbf{E}[X^2] - \mathbf{E}[X]^2
		\end{align}
		You can also think of it as mean of the square of the deviations of original random variable.
		
		One would wonder why $\mathbf{E}[(X- \mathbf{E}[X])^2]$ is used instead of $\mathbf{E}[X - \mathbf{E}[X]]$,
		The reason is $X - \mathbf{E}[X]$ is signed distance, here is a short proof 
		\begin{align}
			\mathbf{E}[X - \mathbf{E}[X]] &= \mathbf{E}[X - k] \\
			            &= \mathbf{E}[X] - \mathbf{E}[k] \\
			            &= k - k \\
			            &= 0
		\end{align} 
	\end{defn}

	\begin{defn}{\textbf{Standard Deviation}}
	It is a measure of dispersion of a random variable. Let $X$ be a random variable. The standard deviation of X, denoted by $stdev[X]$ or $std[X]$ is defined as follows:\\
	\begin{align}
	stdev[X] = \sqrt{Var[X]}
	\end{align}
	You can also think of it as rms deviation.
	\end{defn}


	\begin{defn}{\textbf{Covariance}}
	It is a measure of association between two random variables.
	Covariance of two random variables X and Y is , provided Expected values are well defined,
	\begin{equation}
		Cov[X,Y] = \mathbf{E}[ (X - \mathbf{E}[X]) * (Y - \mathbf{E}[Y]) ] = \mathbf{E}[\overline{X} * \overline{Y}]
	\end{equation}
	where we say deviation of X is $\overline{X} = X - \mathbf{E}[X]$ and \\
	deviation of Y is $\overline{Y} = Y - \mathbf{E}[Y]$\\
	or in other words, covariance is expectation of product of deviations.
	\end{defn}

	\begin{defn}{\textbf{Joint PMFs}}
		Joint probabilities tell how likely it is that certain $X$'s go together with certain $Y$'s. Notation is: \\
		\begin{align}
			p_{X,Y}(x,y) = P(X = x\ and\ Y = y)
		\end{align}
		It looks like following:
		\includegraphics{jointpmf}
		Useful in statistical studies that try to relate
		random variables to each other.
		Some properties are: \\
		\begin{itemize}
			\item $\sum_{x} \sum_{y} p_{X,Y}(x,y) = 1$
			\item Finding PMF for single random variable is simple: $p_X(x) = \sum_{y} p_{X,Y}(x,y)$, these are also known as marginal PMFs.
			\item Conditinal expectation means locking down a row/column as a new unverse, i.e. $p_{X|Y}(x|y) = P(X=x | Y=y) = p_{X,Y}(x,y)/p_Y(y)$
		\end{itemize}
	\end{defn}

	\begin{defn}{\textbf{Random Vector}}
	It is a multidimensional generalisation of the concept of Random Variable. Associated probability functions have the word "joint" in front of them e.g. Joint PMF, Joint cdf etc correspond to a Random Vector.
	\end{defn}

	\section{Logistic}	
	A logistic function or logistic curve is a common 'S' shaped curve with equation:
	
	\begin{equation}
			f(x) = \frac{L}{1 + e^{-k(x-x_0)}}
	\end{equation}

	
	where
	\begin{itemize}
		\item e = natural log
		\item $x_0$ = x-value of sigmoid's midpoint
		\item L = the curve's maximum value
		\item k = the steepness of the curve
	\end{itemize}

	\subsection{Standard Logistic}
	
		\begin{figure}[h]
		\includegraphics[scale=0.65]{sigmoidplot}
		\centering
	\end{figure}
	
	The standard logistic function is the logistic function with parameters given $(k = 1, x_0 = 0, L = 1)$
	i.e.\\
	\begin{equation}
		\sigma(x) = \frac{1}{1 + e^{-x}}
	\end{equation}
	which when plotted looks like\\


	\textbf{Why is logistic function so important?}
	Because it can take any real input $x, (x \in R)$, whereas the output always takes values between 0 and 1, and hence is interpretable as probability.
	
	
	\subsection{Derivative of logistic}
	
	Let's denote the sigmoid function as $\sigma(x) = \dfrac{1}{1 + e^{-x}}$.
	
	The derivative of the sigmoid is $\dfrac{d}{dx}\sigma(x) = \sigma(x)(1 - \sigma(x))$.
	
	Here's a detailed derivation:
	
	
	\begin{align}
	\dfrac{d}{dx} \sigma(x) &= \dfrac{d}{dx} \left[ \dfrac{1}{1 + e^{-x}} \right] \\
	&= \dfrac{d}{dx} \left( 1 + \mathrm{e}^{-x} \right)^{-1} \\
	&= -(1 + e^{-x})^{-2}(-e^{-x}) \\
	&= \dfrac{e^{-x}}{\left(1 + e^{-x}\right)^2} \\
	&= \dfrac{1}{1 + e^{-x}\ } \cdot \dfrac{e^{-x}}{1 + e^{-x}}  \\
	&= \dfrac{1}{1 + e^{-x}\ } \cdot \dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}}  \\
	&= \dfrac{1}{1 + e^{-x}\ } \cdot \left( \dfrac{1 + e^{-x}}{1 + e^{-x}} - \dfrac{1}{1 + e^{-x}} \right) \\
	&= \dfrac{1}{1 + e^{-x}\ } \cdot \left( 1 - \dfrac{1}{1 + e^{-x}} \right) \\
	&= \sigma(x) \cdot (1 - \sigma(x))
	\end{align}
	
	\subsection{Cost function of logistic regression}
	
	Prediction function is sigmoid of weighted sum of input features:
	\begin{align}
		\bm{\hat{y}} = \sigma (\theta^T\cdot \bm{x})
	\end{align}
	
	The log-loss function for a particular prediction is defined as $L(\hat{y},y)$ :
	\begin{align}
		L(\hat{y}, y) = y log\big(\hat{y}\big) + 
		\big(1 - y\big)log\big(1 - \hat{y}\big)
	\end{align}
	
	Cost function for all examples is average of log-loss for each example i of the m examples:
	\begin{align}
		J(\theta) = - \frac{1}{m} \sum_{i=1}^{m} \big[ y^{(i)} log\big(\hat{y}^{(i)}\big) + 
		\big(1 - y^{(i)}\big)log\big(1 - \hat{y}^{(i)}\big)
		\big]
	\end{align}
	
\end{document}